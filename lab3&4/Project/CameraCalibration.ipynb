{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b78389fa-0d2c-4c61-b5fa-036f71ce5cc4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Camera Calibration\n",
    "\n",
    "Before we can gather any information from whatever picture, we ought to obtain the parameters of our sensor(s). Which in this case is the camera. \n",
    "<br/> Therefore, we'll start with the camera calibration using the _chessboard_ provided at the beggining of the assignment.\n",
    "<br/>\n",
    "In short, camera calibration is estimating the parameters of a camera. <br/>\n",
    "- On the one side, we'll need intrinsic parameters like the focal length (f_x, f_y) and optical centers (c_x, c_y), which will be used to create the camera matrix. Which in turn will be used to remove distortion due to the lenses of a specific camera. Each camera having it's own distortion, meaning that once the camera calibration has been calculated we will be able to use said matrix for future operations using the same camera. </br>\n",
    "- On the other side, we'll need extrinsic parameters, meaning rotation and translation vectors which translate coordinate points of a 3D point to a coordinate system.\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### Step 1\n",
    "For starters we'll need to define the size and dimension of our _chessboard_. \n",
    "<br/>\n",
    "It's dimension is defined by the amount of pixels along the width and the lenght of an image, in this case we're working with 1920x1080.\n",
    "And it's size is defined as the number of intersections between the black and grey squares that make up the _chessboard_, which in this case represents a 7x7, which corresponds to the pattern we are looking for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "278d10da-9ac2-41d8-b92a-d55842438457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np \n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import PIL.ExifTags\n",
    "import PIL.Image\n",
    "\n",
    "\n",
    "#chessboard dimensions\n",
    "chessboardSize = (7, 7)\n",
    "frameSize = (1920, 1080)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cf21d9-c2eb-4c65-9a23-9903688103d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2\n",
    "Next, we'll need to define the termination criteria, i.e, the \"Limit on number of iterations: When the limit on number of iterations or number of function evaluations exceeds a specified value, the iteration is terminated\" [^1]. <br/>\n",
    "For this purpose we'll use the two classes TERM_CRITERIA_EPS and TERM_CRITERIA_MAX_ITER: <br/>\n",
    "- TERM_CRITERIA_EPS = stop the algorithm iteration if specified accuracy, epsilon, is reached\n",
    "- TERM_CRITERIA_MAX_ITER = stop the algorithm after the specified number of iterations, max_iter\n",
    "\n",
    "The structure:\n",
    "\n",
    "```\n",
    "TermCriteria(\n",
    "\n",
    "int type, // CV_TERMCRIT_ITER, CV_TERMCRIT_EPS, or both\n",
    "\n",
    "int maxCount,\n",
    "\n",
    "double epsilon\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3bb4aa6-e0ba-4b68-a624-0bf65eb7e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3271de1d-edbb-4294-b6aa-4edc4caf6eea",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "For simplicity's sake, we suppose that the _chessboard_ is stationary and thus we do not need to consider the _Z_ variables (_Z = 0_).</br>\n",
    "With this information in mind, we can already prepare  _XY_ coordinates of object points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70e26f51-30fa-4037-9c1d-50be34784800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "objp = np.zeros((7*7,3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:7,0:7].T.reshape(-1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9feeaf-559a-4b7a-b7f4-4c6dd10e28b7",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "Prepare arrays to store our object points (3D) and image points (2D) and fetch _chessboard_ images with the glob function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8ca60a1-d2a4-41a5-8b6e-9bec6a2a922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d point in real world space\n",
    "imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "#pc\n",
    "images = sorted(glob.glob(r'C:\\Users\\edgar\\Desktop\\Github\\ImageProcessing\\lab3&4\\chessboards\\*.png'))\n",
    "#laptop\n",
    "#images = sorted(glob.glob(r'C:\\Users\\edgar\\Documents\\GitHub\\ImageProcessing\\lab3&4\\chessboards\\*.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fc376d-6447-4280-a7bd-3342d516f0a1",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "Use cv.findChessboardCorners() to find the aforementioned pattern in the _chessboard_. And once the corners have been found, use cv.cornerSubPix() to increase their accuracy. </br></br>\n",
    "Internal workings of cv.findChessboardCorners():\n",
    "<ol>\n",
    "  <li>The input image is first converted to grayscale, as the corner detection algorithms work better on grayscale images.</li>\n",
    "  <li>An edge detection algorithm is applied to the grayscale image to find the edges of the chessboard squares (Canny edge detector). The edges of the squares are used to find the corners of the chessboard pattern.</li>\n",
    "  <li>The corner positions are refined using a corner detection algorithm to get more accurate corner positions (Harris corner detector).</li>\n",
    "</ol>\n",
    "\n",
    "cv.cornerSubPix() uses an iterative algorithm to refine the positions of the corners. It starts with an initial estimate of the corner positions and then refines the estimates using a local optimization procedure (Harris corner detector, Shi-Tomasi corner detector, FAST corner detector, Good Features to Track detector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10b873c4-07e5-4a13-8305-d042378c0427",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in images:\n",
    "    img = cv.imread(fname)\n",
    "    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Find the chess board corners\n",
    "    ret, corners = cv.findChessboardCorners(gray, chessboardSize, None)\n",
    "    \n",
    "    # If found, add object points, image points (after refining them)\n",
    "    if ret == True:\n",
    "        objpoints.append(objp)\n",
    "        corners2 = cv.cornerSubPix(gray,corners, (11,11), (-1,-1), criteria)\n",
    "        imgpoints.append(corners2)\n",
    "        \n",
    "        # Draw and display the corners\n",
    "        cv.drawChessboardCorners(img, chessboardSize, corners2, ret)\n",
    "        cv.imshow('img', img)\n",
    "        cv.waitKey(500)\n",
    "        \n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21073fbb-a5a9-4f93-a858-df4834916f7a",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "Using the cv.calibrateCamera() (=> Zhengyou Zhang pattern analysis and machine intelligence, simulates radial lense), we can start the actual camera calibration. This will create the camera matrix, distortion coefficients, rotation and translation vectors etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab1a7d14-3e88-4792-bb50-da63679625f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, mtx, dist, rvecs, tvecs = cv.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n",
    "\n",
    "#Save parameters into numpy file\n",
    "np.save(r'C:\\Users\\edgar\\Desktop\\Github\\ImageProcessing\\lab3&4\\cameraParams', ret)\n",
    "np.save(r'C:\\Users\\edgar\\Desktop\\Github\\ImageProcessing\\lab3&4\\cameraParams', mtx)\n",
    "np.save(r'C:\\Users\\edgar\\Desktop\\Github\\ImageProcessing\\lab3&4\\cameraParams', dist)\n",
    "np.save(r'C:\\Users\\edgar\\Desktop\\Github\\ImageProcessing\\lab3&4\\cameraParams', rvecs)\n",
    "np.save(r'C:\\Users\\edgar\\Desktop\\Github\\ImageProcessing\\lab3&4\\cameraParams', tvecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4b5842-d00f-4520-9c7f-935d33a1fb74",
   "metadata": {},
   "source": [
    "### Step 7\n",
    "Using the cv.getOptimalNewCameraMatrix() we can refine the camera matrix based on a free scaling parameter. Alpha being the scaling parameter, we can choose it's value based on our application. For instance, if we need to see all the input pixels, then alpha = 1. Otherwise, we choose an alpha that keeps the interesting part of the image inside the undistorted image. This process allows us to undistort the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a97dcc6a-3fb0-46e9-b0a5-76d5fbc84dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pc\n",
    "img = cv.imread(r'C:\\Users\\edgar\\Desktop\\Github\\ImageProcessing\\lab3&4\\chessboards\\c4Left.png')\n",
    "#laptop\n",
    "#img = cv.imread(r'C:\\Users\\edgar\\Documents\\GitHub\\ImageProcessing\\lab3&4\\chessboards\\c4Left.png')\n",
    "\n",
    "h,  w = img.shape[:2]\n",
    "newcameramtx, roi = cv.getOptimalNewCameraMatrix(mtx, dist, (w,h), 1, (w,h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90a32ca2-6f01-4b6d-abaf-2f537cbfb28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.386738059794413\n"
     ]
    }
   ],
   "source": [
    "fovx, fovy, focalLength, principalPoint, aspectRatio = cv.calibrationMatrixValues(mtx, (w, h), 5.6, 5.6)\n",
    "\n",
    "print(focalLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf006fb-9065-4842-b66f-fa57d4a90840",
   "metadata": {},
   "source": [
    "### Step 8\n",
    "Internally, cv.undistort() uses the camera matrix and distortion coefficients to compute the mapping between distorted and undistorted image points. It then uses this mapping to apply a correction to the image, resulting in the undistorted image. (See: Direct linear transformation & Radial distortion correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4e62647-561c-49cc-a0ff-33ceca55dadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# undistort\n",
    "dst = cv.undistort(img, mtx, dist, None, newcameramtx)\n",
    "\n",
    "# crop the image\n",
    "x, y, w, h = roi\n",
    "dst = dst[y:y+h, x:x+w]\n",
    "cv.imwrite('calibresult.png', dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd9f482-2a77-4aee-abba-46d6bff16909",
   "metadata": {
    "tags": []
   },
   "source": [
    "[^1]: https://www.sciencedirect.com/topics/engineering/termination-criterion#:~:text=Termination%20Criteria&text=1.,sense%2C%20the%20iteration%20is%20terminated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
